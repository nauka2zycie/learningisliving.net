<!doctype html>
<html lang="en">
    <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  




<link rel="stylesheet" href="https://learningisliving.net/css/katex.min.css">
<script type="text/javascript" defer src="https://learningisliving.net/js/katex.min.js"></script>
<script type="text/javascript" defer src="https://learningisliving.net/js/katex-auto-render.min.js" onload='renderMathInElement(document.body,{
                                                              delimiters: [
  {left: "\\begin{equation}", right: "\\end{equation}", display: true},
  {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
  {left: "\\begin{align}", right: "\\end{align}", display: true},
  {left: "\\begin{align*}", right: "\\end{align*}", display: true},
  {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
  {left: "\\begin{gather}", right: "\\end{gather}", display: true},
  {left: "\\begin{CD}", right: "\\end{CD}", display: true},
  {left: "$$", right: "$$", display: true},
  {left: "$", right: "$", display: false},
  {left: "\\(", right: "\\)", display: false},
  {left: "\\[", right: "\\]", display: true}
]
  });'></script>




  <meta name="generator" content="Hugo 0.92.0-DEV" />
  


<link rel="stylesheet" href="https://learningisliving.net/css/bootstrap.min.css">


<link rel="stylesheet" type="text/css" href="https://learningisliving.net/css/style.css">



  
  
  <title>The Harm Of 10 Years Of AGI Doomsday | Learning is Living</title>

</head>

  <body>
        <div id="nav-border" class="container">
  
  <nav id="nav" class="nav justify-content-center">
  
  
  
    
    <a class="nav-link" href="https://learningisliving.net/">Home</a>
    
  
  
  
    
    <a class="nav-link" href="https://learningisliving.net/blog">Blog</a>
    
  
  
  
  
  
  
    
    <a class="nav-link" href="https://learningisliving.net/about">More about me</a>
    
  
  
  
  
  </nav>
</div>

    <div class="container">
      <div id="breadcrumbs">


    <a href="https://learningisliving.net/">Home</a>
    
    
        
            
                
                    
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
        
    
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
        
            / <a href="https://learningisliving.net/blog/">Blog</a>
        
    
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
        
            / <a href="https://learningisliving.net/blog/the_harm_of_10_years_of_agi_doomsday/">The harm of 10 years of agi doomsday</a>
        
    
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
                
                    
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
        
    

</div>

      <main id="main">
       

<h1>The Harm Of 10 Years Of AGI Doomsday</h1>
<p>



Started: <time datetime="2023-04-02">2023-04-02</time>

Last Modification: 2023-04-02

, 4717 words, est. reading time: 23 minutes





</p>
<p><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">FLI
published an open letter</a>, led by like Elon Musk (of “apartheid
emerald mine bootstrapper” and “burning 54 GigaDollars to own the libs”
fame) and also some people I respect like Yoshua Bengio, calling on labs
to pause research on large language models, and specifically to pause
increasing the scale of them. Having followed the discussion in then
field of “AI safety” since about 2013, I conjecture that there is a lot
of heterogeneity in the motivations of the signees - some are gonna be
worried about how we <em>use</em> these models, some will worry about
the AI going FOOM and turn us all into paper clips and Elon Musk I’m
gonna assume is just salty because he’s not gonna get a cut of the
OpenAI 100x “limited profit”cap<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>A day later, <a href="https://laion.ai/blog/petition/">LAION launched
a petition</a> calling for the establishment of the “CERN of AI”, a
public institution to research open source ML models, AI safety methods
and, crucially</p>
<blockquote>
<p>100’000 state of the art AI accelerators</p>
</blockquote>
<p>with a public access model in order to allow people to actually train
models toe to toe.</p>
<p>The response, as of 2023-04-02 09:06 is as follows:</p>
<ul>
<li>FLI letter: 2825 signatures</li>
<li>LAION letter: 1090 signatures, including mine</li>
</ul>
<p>The reason for this post, however, is that both in the discussions
I’ve had so far and on the petition website itself, people are arguing
against the establishment of the center on an “existential risk” basis:
AGI has an XX% risk of destroying humanity (between 10 and 100%), so we
shouldn’t do it at all, and if we do it, then it shouldn’t be done in an
open source, public institution fashion.</p>
<p>This, as the kids say, triggered me, and lead me to summarize my big
problem with the Eliezer-Bostromiam AGI doomsday cult once and for all,
so I can just link people to this post.</p>
<p>It has two sections, <a href="#short_summary">part one</a> about some
background to the state of the discussion, <a href="#llms">part two</a>
why AGI isn’t going to happen with the current LLM approaches and people
need to stop overhyping already very transformative tech and <a
href="#even_if">part three</a> which goes onto why <em>even if</em>
there was a chance of us gaining AGI this way, we should still sign the
LAION letter and push for public, open source development and wide
dissemination.</p>
<p><strong>Note as of 2023-04-02</strong>: Might contain typos, require
tightened prose and more sources, and might also be unfair with some
people I’m frustrated with. I plan to revisit this text. Bear with
me.</p>
<p><strong>Edit 2023-04-02 13:28</strong>: fixing some typos and
specifying some claims.</p>
<h1 id="short_summary">A short summary of AGI risk history</h1>
<p>AGI (artificial general intelligence), like the word <a
href="https://en.wikipedia.org/wiki/AI_effect">AI</a> or the notion of
intelligence is fuzzily defined <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>, but in the AGI risk
community generally means one of the following</p>
<ol type="1">
<li>a computer system with close to, or equal to human level
capabilities on all or most cognitive tasks</li>
<li>a computer system with <em>better</em> than human level capabilities
on all tasks</li>
<li>AI Satan, destroyer of worlds, creator of paper clips</li>
</ol>
<p>where the last line is not meant as a dismissive insult, but an
actual description of small parts of the community which mainly engage
in fear based, emotional reasoning and catastrophizing with religious
overtones rather than sober thought about the implications of tech.
However, I need to emphasize, <em>this is the minority</em>. Most
people, when pushed, worry <em>a bit</em> about outcome 2 or 3, but
generally worry about the risks posed by 1, and that’s a good enough
thing to do.</p>
<p>However, some of the loudest and oldest voices in that conversation,
namely the lineage of Nick Bostrom and Eliezer Yudkowsky has spent the
last 10ish years pumping out propaganda focused on the horrible doom
that could befall humanity in case 2 and 3, funded generously by
billionaire endowments out of silicon valley. It’s important to note,
that neither Bostrom nor Yudkowsky have ever worked with or created ML
systems or researched optimization <a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a>, which matters because
while on a high level, you can think about the fixed points of a utility
function of a system and assume it will learn it to optimality,but in
practice, the parametrization and training methods <em>matter</em> and
determine whether you can actually reach that optimum<a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Or
put differently, they make the classic mistake of taking a <em>possible
configuration of the world based on first principles</em> and acting as
if it is a <em>possible configuration in our current
trajectory</em>.</p>
<p>The difference between these two things is that the laws of physics
and social dynamics allow for all types of configurations, but whether
you can actually <em>reach them</em> in the current instance of the
world is a different question - or shorter, I can imagine many plausible
things, but that doesn’t make the realistic.</p>
<p>However, Bostroms whole stick is (paraphrasing his own worlds)
imagining the worst possible outcome and trying to prevent it, which
honestly? I’m glad someone does that.</p>
<p>However, the problem with AGI risk and existential risk in general is
that <em>risk is personal</em>: climate change is an existential risk
for hundreds of millions of people, even in best case scenario. The
<em>species</em> might survive, but for these people, that’s no solace -
they aren’t so much facing existential risk as existential
<em>doom</em>, probability 1 if nothing radical changes.</p>
<p>And at the same time, billionaires like Musk only care about climate
change to get tax credits for overpriced badly manufactured “luxury”
cars - they have enough resources to relocate to a new private Island if
required - their risk of climate change is only nonzero because of
flow-through effects like nuclear war becoming more likely, or the
collapse of the economy.</p>
<p>So for the general public and even millionaires, existential risks
are things like climate change, small scale nuclear war, authoritarian
lock-in, pandemics like COVID where 1-10% of humanity might die etc.</p>
<p>Instead, for billionaires, existential risks are things like AGI,
large scale nuclear war, pandemics that wipe out 60-100% humanity,
global wealth taxes and an unconditional basic income. Everything else
they will either be able to mitigate (see <a
href="https://www.theguardian.com/news/2022/sep/04/super-rich-prepper-bunkers-apocalypse-survival-richest-rushkoff">the
ultra rich preppers here</a>) or they will be the ones benefiting -
authoritarian lock-in is from the powerful to the weak, so it’s not even
a risk if you think you’ll be on the authoritarian side instead of the
locked-in side.</p>
<p>So Bostrom and Yudkowsky spend <em>a lot</em> of words talking about
the doom of runaway computer AI and (in Bostroms case) come up with
“solutions” like pan-optica implemented via necklaces which will stop
you from leaking dangerous information, but discussions of the use of
existing algorithmic systems to influence elections, stop labor from
organizing or even just the eerie similarity of large multinational
corporations and the shareholder system to a profit maximizing runaway
optimizer are <em>conspicously</em> absent in their writing - I’m sure
it has <em>nothing</em> to do with where their funding comes from.</p>
<p>Note, I’m not accusing anyone here of being a mustache twirling
villain, I’m sure everyone involved is a nice human beings<a href="#fn5"
class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
doing what seems right to them - but the whole point of cognitive biases
is that they affect everyone. So risk-salience being different is not
something you can smart your way out of, you need to have plurality of
decision making (i.e., democracy). Self preservation and status quo bias
is strong, so if Bostrom and Yudkowsky get funding for <em>one</em>
aspect of their concerns, they’ll focus on that and stop doing the part
they can’t change and which will dry up their funding - e.g. pointing
out that corporations and states are suspiciously like optimizers,
especially with capital being able to buy lobbying. And finally, humans
soak up a <em>deep</em> narrative pattern of an apocalypse, which is why
<a href="https://en.wikipedia.org/wiki/QAnon">we</a> have <a
href="https://en.wikipedia.org/wiki/Great_Awakening">had</a> a <a
href="https://en.wikipedia.org/wiki/C/2010_X1">bunch</a> of <a
href="https://en.wikipedia.org/wiki/2012_phenomenon">end of the
world</a> scares, partially motivated by <a
href="https://en.wikipedia.org/wiki/Technostress">technostress</a>. If
the world is scary, uncertain and you don’t know how things will go,
boiling things down to a binary helps cope with that complexity. The end
of the world can be a very comforting idea.</p>
<p>Given this, the idea of a big, end of the world scenario is a very
good way to collect funding <em>and</em> a dangerous cognitive trap to
fall into. As I said, I unironically think it’s a good thing a
professional paranoiac like Bostrom exists, but if mixed with
billionares money and a fan-of-AI like Yudkowsky, the mix is an AGI
doomsday cult, where we need to find the right incantation to seal Satan
away before all is too late - and in the histeria this induces, the
question of what happens if AGI <em>isn’t</em> the end of the world is
assumed to be a utopia, since there is only the binary - we find the
incantation there will be utopia, otherwise extinction.</p>
<p>In a mode like this, there is no room for AGI or AI being easy to
control, but that control being abused, or questions of politics and
balance of power between stakeholders - there is only “humanity” and
“the AI”.</p>
<p>The question</p>
<blockquote>
<p>What would you do if the end of the world doesn’t come?</p>
</blockquote>
<p>is completely neglected in these analyses as all being the “good”
scenario.</p>
<p>Now, some good definitely came from Yudkowsky, Bostrom, Stuart
Russel, FHI/FLI etc. engaging with the public on this, and I know people
funded from the same money troughs who are doing lovely work.</p>
<p>But this elitist perspective on AGI risk has severely poisoned the
well and only now with the EFF, Mozilla and other activist groups
getting similar funding we see more sober AI risk being discussed which
- surprise - is mainly of the form “powerful entity or state automates
oppression of poor people”, as in the <a
href="https://www.wired.com/story/welfare-state-algorithms/">Rotterdam
AI report by lighthouse reports</a>.</p>
<p><a
href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225">Carla
Cremer pointed out this problem on how we define risks</a> two years ago
<a
href="https://www.vox.com/future-perfect/23569519/effective-altrusim-sam-bankman-fried-will-macaskill-ea-risk-decentralization-philanthropy">and
has</a> continued to <a
href="https://www.youtube.com/watch?v=lxaTinmKxs0">speak out about
this</a>, and I hope that the discourse will shift towards questions
about legitimacy of control, not just control per se.</p>
<h1 id="llms">Why LLMs are not, and will not become, AGI</h1>
<p>Now, let’s say you buy into the idea of AGI being risky, are LLMs
likely to get us there? Or in general, is fast takeoff (the AI going
FOOM singularity style) a likelihood?</p>
<p>The answer, in brief, is no.</p>
<p>The last 10 years of AI progress has been a mix of maybe 10% or 20%
percent algorithmic improvements and 80% increase in compute and us
getting better and more creative about shoveling more data into ever
larger models. <em>This is not nothing</em> and it’s incredibly useful,
but the debate over <em>AGI</em> requires us to ask whether this
approach can get us there, and the answer, most likely is no, for the
following reasons:</p>
<ol type="1">
<li>We are likely to <a
href="https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset">run
out of training data</a> in form of text and images soon, next up are
audio and video, but then that is kinda it - no more nicely structured
input to memorize and autocomplete…</li>
<li>and <a href="that">that</a> is all these language models do, as
evidences by e.g. the glitch token line of investigations, <a
href="https://evanthebouncy.medium.com/llm-self-play-on-20-questions-dee7a8c63377">the
horrible performance</a> of GPT3.5 on 20 questions and - well, any
critical interaction with the system. They memorize the training set as
a set of feature-chunks (so not a 1:1 copy but useful snippets) and then
regurgitate them in an associative fashion. That is different from
reasoning, from planning, or even from how humans use language which is
tied to a self loop…</li>
<li>which the transformer architecture <em>inherently</em> cannot
express: it is a learned, <span class="math inline">\(8k\)</span>-token
sliding window filter giving you a prediction of the next token given
the input - nothing more, nothing less. If you put in a lot of effort,
you can steer this to be useful by giving inputs <em>you know</em> will
be likely to yield useful autocompletes, but that’s like setting up a
bunch of dominoes so they will spell out “Fish” when tipped over - if
you claim <em>the dominoes</em> or <em>gravity</em> wrote the word, you
are misattributing<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></li>
<li>In order to develop a self loop, you need <em>state</em>, which is
the one thing the transformer architecture tried to do away with because
you need to be stateless for parallelized training</li>
<li>even if you had the capability for a state loop, you would need to
train the model in a way which induces <a
href="https://arxiv.org/abs/1801.04016">causal reasoning</a>, something
which I am increasingly convinced requires an inductive bias towards a
“self”. This is simply because having a notion of self makes it easier
to learn which part of the intervention was <em>you</em> and which part
was the causal consequence</li>
<li>but even if you put all of this into an ML system, you then need to
train it in an online fashion - i.e. one step at a time, as you are
doing things. You can’t easily parallelize interactions with the real
world, you need to keep track of the <a
href="https://arxiv.org/pdf/1802.01561.pdf">off-policy bias</a> and
training becomes unstable easily - and the whole apparatus LLMs relies
on large batch, <em>very careful</em> training in order to not become
unstable</li>
<li>finally, even if you handled all of this, the sample complexity of
learning things in nonstationary environment via reinforcement learning
scales <em>very</em> unfavorably<a href="#fn7" class="footnote-ref"
id="fnref7" role="doc-noteref"><sup>7</sup></a></li>
</ol>
<p>Now, FOOM-AGI riskers will swat away all of these with “but that’s no
proof that it’s impossible”, to which I answer “you have no proof that I
am not god the almighty either”.</p>
<p>This is a community which has spent 10 years rationalizing away the
core logical fallacies of their idea construct namely</p>
<ul>
<li><a
href="https://en.wikipedia.org/wiki/God_of_the_gaps#Usage_in_referring_to_a_type_of_argument">god
of the gaps</a> aka argument from ignorance, i.e. “you can’t prove AGI
is impossible or safe, therefore it’s inevitable and unsafe”<a
href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a></li>
<li><a href="https://en.wikipedia.org/wiki/Pascal%27s_mugging">Pascals
mugging</a> a.k.a. “don’t use probability theory in an open world
setting”. If you try to reason about something using probabilities with
no grounding of it’s impact nor it’s probability, you can just make up
whatever impact and probability your predisposed conclusion needs and go
with it</li>
</ul>
<p>The frame of discourse is now one in which the burden of proof is not
<em>on them</em> to argue that AGI is actually coming and it’s a big
risk, but on <em>you</em> for arguing why we can take the chance,
however slim at AI Satan emerging.</p>
<p><em>That’s</em> why I brought in the AI Satan earlier and why I call
this subset a doomsday cult, because it is a belief system which induces
a form of conspiratorial reasoning and cognitive dissonance.</p>
<p>They keep predicting AI will improve and jump, and any advance is
taken at face value if it is in favor of that narrative, while
counter-examples are reframed as actually proving their point or not
mattering because of some vague “future advancement”.</p>
<p>Examples of this include:</p>
<ul>
<li>the aforementioned glitch tokens (which are just a funny detail of
training, not something about what the models actually learn) and 20
questions (which is relatively new, but just the next challenge to
overcome)</li>
<li>adversarial examples, which uncovered that neural models learn very
simplistic and brittle models, as theorized previously but difficult to
prove? Taken as examples of how little we understand about neural
networks and dangerous unexpected scaling</li>
<li>alphafold/alphacode: the former can be described as a human made
model of protein folding with neural networks in strategic places to
learn heuristics from data - but the discourse describes it as learning
protein folding without human supervision. The latter is a human
constructed system to solve programming puzzles via an LLM - which it
does by generating millions of attempts, filtering it to 100 attempts,
trying them and calling it “solved” if one succeeds, yielding something
like 30% success rate on the test set. This is even less impressive if
you know that competitive programming is basically “recognize which
algorithm you need to implement and do it quickly”, not a real
programming task where you need to break down requirements and design a
system de novo.</li>
<li>the line of work on mis-generalization in RL: it has been long known
that if you do not randomize your environment and regularize your RL
agents, they will learn shortcut solutions to a given task. The papers
in question do not do this, and then describe the failure to learn the
task as given as “mis-generalization”. There is no generalisation here,
the thing just failed to learn your task.</li>
<li>language models can be goaded into competing sequences according to
a pattern you give to them, e.g. if you count “1 2 fish 4 5 fish 7 8
fish” it has a decent chance of continuing the sequence “10 11 fish”
etc.</li>
<li>This makes sense, since they are trained to sample from <span
class="math inline">\(p(x_{t+1}|x_{t},\dots,x_1)\)</span>. However, this
has now been termed “in context learning” (a redefinition of the
learning term since no parameters are updated) and the community is now
similarly discussing “simulator theory”, “the wa-luigi effect” and a
whole bunch of other new-speak. All because they are set on these models
being more powerful than just a language autocomplete with some text
snippets memorized. The (actually very nice!) insight “polite people are
always polite, rude people are sometimes polite” could used to frame
things in terms of conditional probabilities - e.g. thinking on how to
frame prompts such that <em>in your dataset</em> the prompt would come
from reasonable discourse.</li>
<li>Instead they talk about it in terms of “summoning the simulacra” of
an evil persona - which is an incredibly dramatic way to express “the
model has associates input which is sometimes rude as having high
probability of rude output”.</li>
</ul>
<p>It’s a generation of AI enthusiasts which has self-indoctrinated
themselves to look for the AGI in systems which, taken soberly <em>do
not exhibit it</em> and this sloppy discourse has now started to <a
href="https://arxiv.org/abs/2303.12712">affect even researchers I
respect immensely</a> <a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a>.</p>
<p><a
href="http://proceedings.mlr.press/v119/yehuda20a/yehuda20a.pdf">It is
impossible to train an algorithm to solve NP-hard problems</a> with
polynomial samples. <a
href="https://openreview.net/forum?id=9NjqD9i48M">Four month old Infants
outperform ML systems at generalization</a> While you might be able to
train an AGI <em>some day</em> and we will have more and more advanced
systems which will be incredibly useful, we do not have a roadmap to
AGI.</p>
<p>And I make a testable prediction that anyone who tells you otherwise
in the current discourse is either</p>
<ul>
<li>working for a company benefiting from AI hype</li>
<li>pushing their own research agenda on capabilities or safety</li>
<li>is trying to lobby for regulatory capture/exclusion of the public to
source code, data and models</li>
</ul>
<p>where the last point breaks down into a) well meaning people without
an understanding of AGI trying to be extra cautious for <em>their</em>
risks (emotional reasoning) and b) malicious actors who want to
monopolize power or c) people who got tech-socialized in Bostroms and
Yudkowskys horror universe of paperclip optimizers and Voldemort-super
intelligence.</p>
<h1 id="even_if">Why AI should be publicly controlled, open source and
widely spread <em>even</em> if it were likely and <em>even</em> if we
were close</h1>
<p><strong>TL;DR; intelligence alone doesn’t buy you much, balance of
power, security through obscurity does not work and we did all of this
during the crypto wars already.</strong></p>
<p>Now, with this background, let’s get back to LAION and the CERN of
AI.</p>
<p>Keep in mind we are already in a world where OpenAI, Conjecture etc.
exist, all for profit companies<a href="#fn10" class="footnote-ref"
id="fnref10" role="doc-noteref"><sup>10</sup></a>. What also exists:
Pangu, a trillion parameter Chinese state LLM and Russia, currently
rogue state invading Ukraine and a whole slew of other proprietary tech
companies using AI systems for state and corporate control.</p>
<p>Who would argue against the existence of a public institution that
freely shares models, data and know-how in an effort to commodify and
really democratize<a href="#fn11" class="footnote-ref" id="fnref11"
role="doc-noteref"><sup>11</sup></a> AI access? Notably, an institution
that calls for AI safety to be a first principle and which would
actually develop <em>open</em> AI so people can actually check and not
rely on “trust us bros, we asked our friends at this other Nonprofit and
they say it’s safe”. Who would do that?</p>
<p>Well, I already started this essay with an answer, but just to give
you my assessment summarized</p>
<ul>
<li>actors who stand to make billions of dollars from artificially
monopolizing the tech and want to do regulatory capture</li>
<li>AGI existential riskers who</li>
<li>Ayn Rand like “the state should only protect the property of those
who have it, not any other rights” anti-democracy dudes (it <em>is</em>
mainly dudes <em>for some reason</em>).</li>
</ul>
<p>AI, AGI, whatever you want to call it <em>will</em> affect the
balance of power, most importantly that between <em>labor</em> and
<em>capital</em>. In the absence of UBI and strong political commitments
to democracy, the only avenue we have is to commodify AI models as
quickly as possible, to minimize the comparative advantage between small
companies and individuals and billionaires with hundred of GPUs.
Otherwise, those already in power<a href="#fn12" class="footnote-ref"
id="fnref12" role="doc-noteref"><sup>12</sup></a> hoover up the
efficiency gains of this new tech, except that unions won’t have any
leverage any more to distribute the wages <a
href="https://doi.org/10.1111/kykl.12128">like they did previously</a>
and they will have staggeringly more powerful surveillance and protest
busting capabilities.</p>
<p>So that’s reason one why we need to have public access to the ability
to develop, train and deploy advanced AI and AGI systems: otherwise, we
destabilize the balance of power in a way which enables authoritarian
lock-in.</p>
<p>Reason two is closely linked to this, and that is that inAI/AGI
<em>alone</em> doesn’t buy you much. All the threat-models I am aware of
rely on you having access to some other resources, which are
<em>already</em> the bottleneck and will become so even more with AGI
around</p>
<ul>
<li>want to influence the masses? You need a platform, not content,
content is already cheap</li>
<li>want to phish people? You need insider info to do spear phishing
and/or an anonymous email and cash extraction system</li>
<li>want to manipulate stocks? You need capital, pass KYC checks
etc</li>
<li>want to hack a power grid? You probably need physical access</li>
</ul>
<p>Maciej Cegłowski <a
href="https://www.youtube.com/watch?v=kErHiET5YPw">coined this last one
the “argument from hawkings cat”</a> which sadly isn’t respected as an
argument by AGI riskers, partially because it doesn’t respect the
faux-serious tone adopted by people in a community which unironically
uses “the waluigi effect” as a term…</p>
<p>That is to say, I consider the argument valid and <em>for some
reason</em>, nerds like me who derive a good chunk of our self worth
from what is called “intelligence” tend to overestimate how much power
“intelligence” gives you<a href="#fn13" class="footnote-ref"
id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>Reason three is that without public access to the models, we can’t
find the failure modes and we will go back to redlining. <a
href="https://www.lighthousereports.com/suspicion-machines-methodology/">The
lighthouse reports</a> behind the scenes notes that without access to
the model (kudos to Rotterdam!) they couldn’t have uncovered what they
did - and no actor is as motivated to find failures in their models as
those affected by them. If you are not the one being acted on, you can
mitigate risk by shifting it, hiding it, lobbying against having to pay
damages…everything we see right now with the oil industry and the
climate crisis. Privatize gains, democratize externalities.</p>
<p>The only way to make <em>high quality</em> and <em>safe</em> models a
priority is to have the ability to probe them for failures and make them
public - something that we <a
href="https://en.wikipedia.org/wiki/Coordinated_vulnerability_disclosure">know
in software</a> for decades now, if you allow actors to engage in
[security theater](https://en.wikipedia.org/wiki/Security_theater] they
will, because it’s cheaper.</p>
<p>The final reason is the same line of arguments as from the <a
href="https://en.wikipedia.org/wiki/Crypto_Wars">Crypto wars</a>, where
powerful actors try to keep a monopoly on strong crypto because “bad
people” (terrorists, communists, capitalists, you know “the enemy”)
might use them.</p>
<p>The reply to this, and similar arguments in the ML world is: these
people will get access anyway, and then they will encounter systems with
not defenses against them, and be at an advantage vs. the common good
person.</p>
<p>If you try to stop terrorist from creating neurotoxins by denying
them optimization model to do so, then your failure mode is them getting
the model, using the supply chain and the publics’ gullibility and
unawareness of this risk to get the ingredients and creating them.</p>
<p>If you instead have every high school teacher show how easy it is to
get the formula from one of these models to drive home the point of
<em>why</em> there is so much bureaucracy around neurotoxin ingredients
and why it’s important follow it and report deviances, your failure
model is a whole system of caution malfunctioning…and you have a whole
team of white hats developing counter-toxins, maybe even making whole
classes of toxins obsolete (examples from the software world are modern
defenses against SQL injection or buffer overflows).</p>
<p>Having only states, the rich and the bad guys who side step states
have access to powerful tools robs us of our ability to defend ourselves
and maintain a democracy in which the people are truly sovereign.<a
href="#fn14" class="footnote-ref" id="fnref14"
role="doc-noteref"><sup>14</sup></a></p>
<p>Even <em>if</em> we are talking about AI and AGI tools. And that’s
why you should sign <a href="https://laion.ai/blog/petition/">LAIONs
petition</a>.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>I really wonder how long it will take for other for
profit entities to convert to Non-Profits with a “limited profit
partnership” using 1000x profit caps …seems like a nice way to not pay
taxes on your R&amp;D<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Although François Chollet has the best writing that I am
aware of on this matter <a
href="https://arxiv.org/abs/1911.01547">here</a><a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Yudkowsky had a failed attempt at coding up an XML-lisp
based thing a few decades ago, but his main claims to fame are getting
money for a foundation by being friends with Peter Thiel and other
Silicon Valley techbros, blogging with Robin Hanson and writing a Harry
Potter fanfic).<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The famous universal approximation theorem guarantees
you the existence of a good approximation, not being able to find it<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Yudkowsky in particular is on the record for
<em>explicitly</em> disavowing eugenics, race realists and other dog
whistles for scientific racism, unlike <a
href="https://imgur.com/a/gWeIK6c">other people in the milieu</a> (for
more on Siskind see e.g. <a
href="https://www.eruditorumpress.com/blog/the-beigeness-or-how-to-kill-people-with-bad-writing-the-scott-alexander-method">here</a>)<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>For what it’s worth, I think another bad inductive bias
is the way token mixing is currently implemented in sequence models - at
least as best as I can tell, human thinking isn’t sequential, even if it
sometimes feels like it, we rerun things we read and hear immediately
through a graph of associations closer to a <a
href="https://en.wikipedia.org/wiki/Rhizome_(philosophy)">rhizome</a>
and use anti-causal information flow, while in a transformer, every
token produced only possesses information from the past<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>If <span class="math inline">\(s,a\)</span> and <span
class="math inline">\(h\)</span> are your state space size, action space
size and time horizon respectively, the best bounds I know are <a
href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9719704">either</a>
<span class="math inline">\(\mathcal{O}(sa)^{\mathcal{O}(s)}\)</span>
(independent of <span class="math inline">\(h\)</span> but when you see
<span class="math inline">\(s^s\)</span> you know you are in for a bad
time) or <a href="https://arxiv.org/abs/2204.05275">something like</a>
<span class="math inline">\(\mathcal{O}(h^4s)\)</span> with a bad
dependence on your discount factor (<span
class="math inline">\(\gamma^{-3}\)</span>). For off-policy methods <a
href="https://arxiv.org/abs/2208.03247v2">the critical parameter looks a
bit different</a>, namely the effective horizon <span
class="math inline">\(e=(1-\gamma)^{-1}\)</span> appears as <span
class="math inline">\(e^7\)</span>, meaning going from <span
class="math inline">\(\gamma=0.8\)</span> (decays to <span
class="math inline">\(\approx 0.01\)</span> in 20 steps) to <span
class="math inline">\(\gamma=0.9\)</span> (<span
class="math inline">\(\approx 0.13\)</span> after 20 steps, 42 steps to
<span class="math inline">\(\approx 0.01\)</span>) increases the factor
contributed <em>just</em> by this from <span
class="math inline">\(78e3\)</span> to <span
class="math inline">\(10e6\)</span>, a <span
class="math inline">\(128x\)</span> increase. Long horizon planning
(say, 100 steps in the future still matter, for which we need <span
class="math inline">\(\gamma\geq\approx 0.99895\)</span>) would increase
the number of samples by 15 <em>orders of magnitude</em><a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Mixed with the ever popular tropes of extrapolating the
rising part of a sigmoid and boldly claiming that <a
href="https://www.nber.org/system/files/working_papers/w13882/w13882.pdf">this
time is different</a><a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Although it needs to be said that this is a Microsoft
research paper and Microsoft of course has an interest in hyping LLMs
due to their investment in OpenAI<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Yes, I count OpenAI. See first footnote.<a
href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>In the “equal access for everyone”, not in the “if you
can pay for it you can come” British “public” school way<a
href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Out of an abundance of caution, <a
href="https://en.wikipedia.org/wiki/Antisemitism#Conspiracy_theories">no,
not these</a>, piss of, <a
href="https://en.wikipedia.org/wiki/Old_money">these</a>, <a
href="https://en.wikipedia.org/wiki/New_money">these</a> and <a
href="https://en.wikipedia.org/wiki/Royalty">these</a><a href="#fnref12"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Joining a long lineage of nerds stretching back to
plato who argue that in a perfect society, <em>smart people</em> should
hold all power. For some reason, the question why Donald Trump got
elected if intelligence is so dangerous doesn’t seem to come up much.<a
href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>You might want to talk about handguns, mass shootings
etc. and to this I say: Switzerland has similar gun laws to the US, but
much less shootings. If your society treats its constituents better,
them having power is not a risk. You might also talk about the risk of
rogue states and nuclear weapons, and to this I say a) good thing we are
talking about a CERN for AI and a vending machine for weapons grade
uranium, check the 2nd section and b) while I still think nuclear
disarmament would be good, I also think that if Ukraine hadn’t disarmed
they probably wouldn’t have been invaded last year<a href="#fnref14"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>



      </main>
    <footer>
      

      <footer>
  

</footer>


    </footer>
    </div>
  </body>
</html>
